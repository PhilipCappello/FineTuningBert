{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3357b916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-05 15:48:24.029895: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd63ec5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-05 15:48:25.045726: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-05 15:48:25.046816: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-03-05 15:48:25.103294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-05 15:48:25.103426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1070 computeCapability: 6.1\n",
      "coreClock: 1.7845GHz coreCount: 15 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 238.66GiB/s\n",
      "2022-03-05 15:48:25.103439: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-03-05 15:48:25.104586: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-03-05 15:48:25.104611: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-03-05 15:48:25.105795: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-03-05 15:48:25.105975: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-03-05 15:48:25.107270: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-03-05 15:48:25.107888: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-03-05 15:48:25.110508: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-03-05 15:48:25.110604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-05 15:48:25.110749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-05 15:48:25.110827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-03-05 15:48:25.110848: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-03-05 15:48:25.471273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-03-05 15:48:25.471298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-03-05 15:48:25.471303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-03-05 15:48:25.471495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-05 15:48:25.471691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-05 15:48:25.471803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-05 15:48:25.471895: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/device:GPU:0 with 6927 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "2022-03-05 15:48:25.472134: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "# Get the GPU device name.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# The device name should look like the following:\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c288713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce GTX 1070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc3eaae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-05-21</td>\n",
       "      <td>START BITCOIN MINING       https://t.co/Na1gSB...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>05/27 13:10 ÁèæÂú®„ÅÆ„Éì„ÉÉ„Éà„Ç≥„Ç§„É≥„ÅÆ‰æ°Ê†º\\nBTC/JPY ask: 956,532...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>BTC\\n\\nÊó•Ë∂≥„É¨„Éô„É´„ÅÆË©±„Çí„Åó„Åæ„Åô„ÄÇ\\n\\nÈÅéÂéª‰∏âÂõû„ÅÆ„ÉÅ„É£„É¨„É≥„Ç∏„ÅßBTC„ÅØ‰∏ÄÂ∫¶„ÇÇ„Åì„ÅÆ„Éê„É≥„Éâ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2019-05-21</td>\n",
       "      <td>DAO: Ready? Steady? Go? | BTC-ECHO https://t.c...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2019-05-21</td>\n",
       "      <td>https://t.co/q1l2Ho1ZMX Ethereum Still Bullish...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        Date                                               text  \\\n",
       "0           0  2019-05-21  START BITCOIN MINING       https://t.co/Na1gSB...   \n",
       "1           1  2019-05-27  05/27 13:10 ÁèæÂú®„ÅÆ„Éì„ÉÉ„Éà„Ç≥„Ç§„É≥„ÅÆ‰æ°Ê†º\\nBTC/JPY ask: 956,532...   \n",
       "2           2  2019-05-27  BTC\\n\\nÊó•Ë∂≥„É¨„Éô„É´„ÅÆË©±„Çí„Åó„Åæ„Åô„ÄÇ\\n\\nÈÅéÂéª‰∏âÂõû„ÅÆ„ÉÅ„É£„É¨„É≥„Ç∏„ÅßBTC„ÅØ‰∏ÄÂ∫¶„ÇÇ„Åì„ÅÆ„Éê„É≥„Éâ...   \n",
       "3           3  2019-05-21  DAO: Ready? Steady? Go? | BTC-ECHO https://t.c...   \n",
       "4           4  2019-05-21  https://t.co/q1l2Ho1ZMX Ethereum Still Bullish...   \n",
       "\n",
       "  Sentiment  \n",
       "0  Negative  \n",
       "1  Positive  \n",
       "2  Positive  \n",
       "3  Negative  \n",
       "4  Negative  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the training data into df \n",
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv('CSVFiles/training-data-positive-negative.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd52586e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aafd7c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Sentiment Score')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x=df.Sentiment)\n",
    "plt.xlabel('Sentiment Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6e826ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface in /home/phil81/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (0.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "# install hugging face library to use their transformers package \n",
    "!pip install huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5d72644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 10,000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0])) # we have 10,006 total sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd13ef86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9953/2405270951.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.Sentiment[df.Sentiment == \"Positive\"]=1\n",
      "/tmp/ipykernel_9953/2405270951.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.Sentiment[df.Sentiment == \"Negative\"]=0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4350</th>\n",
       "      <td>4350</td>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>Axmet, Islam, Nurlan, Dawlet, Batir, Sultan, B...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>1296</td>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>Chump change  #sugarbabywanted #sugarbabyneede...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8059</th>\n",
       "      <td>8059</td>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>#PRUX / #BTC Trading is back Online, rise to m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8573</th>\n",
       "      <td>8573</td>\n",
       "      <td>2019-05-21</td>\n",
       "      <td>Fnality: neues Blockchain-Projekt von Credit S...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1289</th>\n",
       "      <td>1289</td>\n",
       "      <td>2019-05-21</td>\n",
       "      <td>don't miss out on registering on Binance, befo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7593</th>\n",
       "      <td>7593</td>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>$BTCUSD üö®  BTC/USD Forex Signal https://t.co/8...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9439</th>\n",
       "      <td>9439</td>\n",
       "      <td>2019-05-21</td>\n",
       "      <td>Token Daily ‚Äì Bitcoin vaudra bient√¥t z√©ro #blo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4069</th>\n",
       "      <td>4069</td>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>#FREE #Ethereum #game\\nhttps://t.co/RrNekTLe88...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7292</th>\n",
       "      <td>7292</td>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>Bitcoin vystoupil na roƒçn√≠ maximum https://t.c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5856</th>\n",
       "      <td>5856</td>\n",
       "      <td>2019-05-21</td>\n",
       "      <td>\"ETF? What ETF? Bitcoin Shrugs Off VanEck Dela...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0        Date  \\\n",
       "4350        4350  2019-05-27   \n",
       "1296        1296  2019-05-27   \n",
       "8059        8059  2019-05-27   \n",
       "8573        8573  2019-05-21   \n",
       "1289        1289  2019-05-21   \n",
       "7593        7593  2019-05-27   \n",
       "9439        9439  2019-05-21   \n",
       "4069        4069  2019-05-27   \n",
       "7292        7292  2019-05-27   \n",
       "5856        5856  2019-05-21   \n",
       "\n",
       "                                                   text Sentiment  \n",
       "4350  Axmet, Islam, Nurlan, Dawlet, Batir, Sultan, B...         1  \n",
       "1296  Chump change  #sugarbabywanted #sugarbabyneede...         1  \n",
       "8059  #PRUX / #BTC Trading is back Online, rise to m...         1  \n",
       "8573  Fnality: neues Blockchain-Projekt von Credit S...         0  \n",
       "1289  don't miss out on registering on Binance, befo...         0  \n",
       "7593  $BTCUSD üö®  BTC/USD Forex Signal https://t.co/8...         1  \n",
       "9439  Token Daily ‚Äì Bitcoin vaudra bient√¥t z√©ro #blo...         0  \n",
       "4069  #FREE #Ethereum #game\\nhttps://t.co/RrNekTLe88...         1  \n",
       "7292  Bitcoin vystoupil na roƒçn√≠ maximum https://t.c...         1  \n",
       "5856  \"ETF? What ETF? Bitcoin Shrugs Off VanEck Dela...         0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = df.replace({'Sentiment': {'Negative':int(0), 'Neutral':int(1), 'Positive':int(2)}})  \n",
    "df.Sentiment[df.Sentiment == \"Positive\"]=1 \n",
    "# df.Sentiment[df.Sentiment == \"Neutral\"]=1 \n",
    "df.Sentiment[df.Sentiment == \"Negative\"]=0 \n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7cbfd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# now we want to format our data so that VERT can use it for training \n",
    "from transformers import BertTokenizer \n",
    "\n",
    "# load the BERT tokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7671e474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  START BITCOIN MINING       \n",
      "Tokenized:  ['start', 'bit', '##co', '##in', 'mining']\n",
      "Token IDs:  [2707, 2978, 3597, 2378, 5471]\n"
     ]
    }
   ],
   "source": [
    "# some regex to clean the data before passing it to BERT tokenizer  \n",
    "import re \n",
    "df = df.replace(to_replace='https?:\\/\\/\\S+', value='', regex=True) # remove https urls \n",
    "df = df.replace(to_replace=\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", value='', regex=True) # remove www urls \n",
    "df = df.replace(to_replace=\"\\[video\\]\", value='', regex=True) # remove videos that got replaced with [video]\n",
    "df = df.replace(to_replace='{link}', value='', regex=True) # remove links \n",
    "df = df.replace(to_replace='&[a-z]+;', value='', regex=True) # remove HTML embedded characters\n",
    "df = df.replace(to_replace='@[^ ]+', value='', regex=True) # remove @usernames \n",
    "\n",
    "# some sample output \n",
    "tweets = df.text.values \n",
    "labels = df.Sentiment.values \n",
    "# import numpy as np\n",
    "# labels = np.array(labels)\n",
    "\n",
    "print(' Original: ', tweets[0])\n",
    "print('Tokenized: ', tokenizer.tokenize(tweets[0]))\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(tweets[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84df8866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-05-21</td>\n",
       "      <td>START BITCOIN MINING</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>05/27 13:10 ÁèæÂú®„ÅÆ„Éì„ÉÉ„Éà„Ç≥„Ç§„É≥„ÅÆ‰æ°Ê†º\\nBTC/JPY ask: 956,532...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>BTC\\n\\nÊó•Ë∂≥„É¨„Éô„É´„ÅÆË©±„Çí„Åó„Åæ„Åô„ÄÇ\\n\\nÈÅéÂéª‰∏âÂõû„ÅÆ„ÉÅ„É£„É¨„É≥„Ç∏„ÅßBTC„ÅØ‰∏ÄÂ∫¶„ÇÇ„Åì„ÅÆ„Éê„É≥„Éâ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-05-21</td>\n",
       "      <td>DAO: Ready? Steady? Go? | BTC-ECHO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-21</td>\n",
       "      <td>Ethereum Still Bullish With Record Trading Vo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>2019-05-21</td>\n",
       "      <td>CRYPTO NEWS: SEC again postpones decision on a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>Received an amazing email from one of my membe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>#bitcoin #(btc) #price #rallies #10%: #bulls #...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>#xrp #ripple #trx #btc #Some Think First 'Gam...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>2019-05-21</td>\n",
       "      <td>hahaha I know you are smarter than thatü§î</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date                                               text  Sentiment\n",
       "0     2019-05-21                        START BITCOIN MINING                 0\n",
       "1     2019-05-27  05/27 13:10 ÁèæÂú®„ÅÆ„Éì„ÉÉ„Éà„Ç≥„Ç§„É≥„ÅÆ‰æ°Ê†º\\nBTC/JPY ask: 956,532...          1\n",
       "2     2019-05-27  BTC\\n\\nÊó•Ë∂≥„É¨„Éô„É´„ÅÆË©±„Çí„Åó„Åæ„Åô„ÄÇ\\n\\nÈÅéÂéª‰∏âÂõû„ÅÆ„ÉÅ„É£„É¨„É≥„Ç∏„ÅßBTC„ÅØ‰∏ÄÂ∫¶„ÇÇ„Åì„ÅÆ„Éê„É≥„Éâ...          1\n",
       "3     2019-05-21                DAO: Ready? Steady? Go? | BTC-ECHO           0\n",
       "4     2019-05-21   Ethereum Still Bullish With Record Trading Vo...          0\n",
       "...          ...                                                ...        ...\n",
       "9995  2019-05-21  CRYPTO NEWS: SEC again postpones decision on a...          0\n",
       "9996  2019-05-27  Received an amazing email from one of my membe...          1\n",
       "9997  2019-05-27  #bitcoin #(btc) #price #rallies #10%: #bulls #...          1\n",
       "9998  2019-05-27   #xrp #ripple #trx #btc #Some Think First 'Gam...          1\n",
       "9999  2019-05-21           hahaha I know you are smarter than thatü§î          0\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(['Unnamed: 0'],axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9819325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence length:  222\n"
     ]
    }
   ],
   "source": [
    "max_len = 0 \n",
    "\n",
    "for i in range(len(tweets)):\n",
    "    # tokenize the text and add `[CLS]` and `[SEP]` tokens \n",
    "    input_ids = tokenizer.encode(str(tweets[i]), add_special_tokens=True)\n",
    "    \n",
    "    #update max sentence length\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "    \n",
    "print('max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86bb16fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phil81/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2251: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  START BITCOIN MINING       \n",
      "Token IDs: tensor([ 101, 2707, 2978, 3597, 2378, 5471,  102,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0])\n"
     ]
    }
   ],
   "source": [
    "# we will set max_len to 235 \n",
    "# now we can perform tokenization \n",
    "input_ids = [] \n",
    "attention_masks = [] \n",
    "\n",
    "for i in range(len(tweets)):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        str(tweets[i]),                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    # add encoded sentence to list \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    # add attention masks to list \n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "#convert the lists into tensors \n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "print('Original: ', tweets[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9b1e3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will not divide our training data into training and validation sets (90/10)\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e91f46b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9,000 training samples\n",
      "1,000 validation samples\n"
     ]
    }
   ],
   "source": [
    "# set batch size to 32 for fine-tuning BERT \n",
    "batch_size = 32 \n",
    "\n",
    "# combine training inputs into a training dataset \n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "training_size = int(0.9*len(dataset))\n",
    "validation_size = len(dataset) - training_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [training_size, validation_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(training_size))\n",
    "print('{:>5,} validation samples'.format(validation_size))\n",
    "# THIS CONCLUDES PROCESSING OUR DATA FOR TRAINING \n",
    "# https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=6J-FYdx6nFE_ \n",
    "\n",
    "# create an iterator for our dataset using the torch DataLoader class, saving on memory during training and boost training speed \n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                                 sampler=RandomSampler(train_dataset), \n",
    "                                 batch_size=batch_size)\n",
    "validation_dataloader = DataLoader(val_dataset, \n",
    "                                   sampler=RandomSampler(val_dataset), \n",
    "                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdc0111e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6795fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e68cc41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phil81/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01389379",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5441908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function for calculating accuracy.\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83ed5976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for formatting elapsed times as `hh:mm:ss`\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cbd3e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    282.    Elapsed: 0:00:13.\n",
      "  Batch    80  of    282.    Elapsed: 0:00:26.\n",
      "  Batch   120  of    282.    Elapsed: 0:00:39.\n",
      "  Batch   160  of    282.    Elapsed: 0:00:52.\n",
      "  Batch   200  of    282.    Elapsed: 0:01:05.\n",
      "  Batch   240  of    282.    Elapsed: 0:01:18.\n",
      "  Batch   280  of    282.    Elapsed: 0:01:31.\n",
      "\n",
      "  Average training loss: 0.67\n",
      "  Training epcoh took: 0:01:32\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'validation_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9953/4189595896.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;31m# Evaluate data for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalidation_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;31m# Unpack this training batch from our dataloader.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'validation_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # In PyTorch, calling `model` will in turn call the model's `forward` \n",
    "        # function and pass down the arguments. The `forward` function is \n",
    "        # documented here: \n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
    "        # The results are returned in a results object, documented here:\n",
    "        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
    "        # Specifically, we'll get the loss (because we provided labels) and the\n",
    "        # \"logits\"--the model outputs prior to activation.\n",
    "        result = model(b_input_ids, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=b_input_mask, \n",
    "                       labels=b_labels,\n",
    "                       return_dict=True\n",
    "                      )\n",
    "\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            result = model(b_input_ids, \n",
    "                           token_type_ids=None, \n",
    "                           attention_mask=b_input_mask,\n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "\n",
    "        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
    "        # output values prior to applying an activation function like the \n",
    "        # softmax.\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ca36032e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "836989c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_input_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "616a9bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "329c97ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66734e96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
