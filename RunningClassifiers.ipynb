{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69e84960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 10:00:00.247700: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-04-16 10:00:01.368478: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-16 10:00:01.370179: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-04-16 10:00:01.401296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-16 10:00:01.401640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1070 computeCapability: 6.1\n",
      "coreClock: 1.7845GHz coreCount: 15 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 238.66GiB/s\n",
      "2022-04-16 10:00:01.401674: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-04-16 10:00:01.404049: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-04-16 10:00:01.404106: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-04-16 10:00:01.406407: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-04-16 10:00:01.406809: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-04-16 10:00:01.409343: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-04-16 10:00:01.410782: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-04-16 10:00:01.415220: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-04-16 10:00:01.415399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-16 10:00:01.415754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-16 10:00:01.416009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-04-16 10:00:01.416057: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-04-16 10:00:01.786365: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-04-16 10:00:01.786385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-04-16 10:00:01.786390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-04-16 10:00:01.786536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-16 10:00:01.786685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-16 10:00:01.786796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-16 10:00:01.786887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/device:GPU:0 with 5725 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "2022-04-16 10:00:01.787089: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce GTX 1070\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "# Get the GPU device name.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# The device name should look like the following:\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')\n",
    "    \n",
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "323c95c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "df_new = pd.read_csv('CSVFiles/new-tweets.csv')\n",
    "df_new_added_old_negative = pd.read_csv('CSVFiles/new-tweets-added-old-negative.csv')\n",
    "df_old = pd.read_csv('CSVFiles/old-tweets.csv')\n",
    "df_old_added_neutral = pd.read_csv('CSVFiles/old-tweets-added-neutral.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26e5f115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer \n",
    "\n",
    "# load the BERT tokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa13f8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some regex to clean the data before passing it to BERT tokenizer\n",
    "import re \n",
    "df_new = df_new.replace(to_replace='https?:\\/\\/\\S+', value='', regex=True) # remove https urls \n",
    "df_new = df_new.replace(to_replace=\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", value='', regex=True) # remove www urls \n",
    "df_new = df_new.replace(to_replace=\"\\[video\\]\", value='', regex=True) # remove videos that got replaced with [video]\n",
    "df_new = df_new.replace(to_replace='{link}', value='', regex=True) # remove links \n",
    "df_new = df_new.replace(to_replace='&[a-z]+;', value='', regex=True) # remove HTML embedded characters\n",
    "df_new = df_new.replace(to_replace='@[^ ]+', value='', regex=True) # remove @usernames \n",
    "\n",
    "df_new_added_old_negative = df_new_added_old_negative.replace(to_replace='https?:\\/\\/\\S+', value='', regex=True) # remove https urls \n",
    "df_new_added_old_negative = df_new_added_old_negative.replace(to_replace=\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", value='', regex=True) # remove www urls \n",
    "df_new_added_old_negative = df_new_added_old_negative.replace(to_replace=\"\\[video\\]\", value='', regex=True) # remove videos that got replaced with [video]\n",
    "df_new_added_old_negative = df_new_added_old_negative.replace(to_replace='{link}', value='', regex=True) # remove links \n",
    "df_new_added_old_negative = df_new_added_old_negative.replace(to_replace='&[a-z]+;', value='', regex=True) # remove HTML embedded characters\n",
    "df_new_added_old_negative = df_new_added_old_negative.replace(to_replace='@[^ ]+', value='', regex=True) # remove @usernames \n",
    "\n",
    "df_old = df_old.replace(to_replace='https?:\\/\\/\\S+', value='', regex=True) # remove https urls \n",
    "df_old = df_old.replace(to_replace=\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", value='', regex=True) # remove www urls \n",
    "df_old = df_old.replace(to_replace=\"\\[video\\]\", value='', regex=True) # remove videos that got replaced with [video]\n",
    "df_old = df_old.replace(to_replace='{link}', value='', regex=True) # remove links \n",
    "df_old = df_old.replace(to_replace='&[a-z]+;', value='', regex=True) # remove HTML embedded characters\n",
    "df_old = df_old.replace(to_replace='@[^ ]+', value='', regex=True) # remove @usernames \n",
    "\n",
    "df_old_added_neutral = df_old_added_neutral.replace(to_replace='https?:\\/\\/\\S+', value='', regex=True) # remove https urls \n",
    "df_old_added_neutral = df_old_added_neutral.replace(to_replace=\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", value='', regex=True) # remove www urls \n",
    "df_old_added_neutral = df_old_added_neutral.replace(to_replace=\"\\[video\\]\", value='', regex=True) # remove videos that got replaced with [video]\n",
    "df_old_added_neutral = df_old_added_neutral.replace(to_replace='{link}', value='', regex=True) # remove links \n",
    "df_old_added_neutral = df_old_added_neutral.replace(to_replace='&[a-z]+;', value='', regex=True) # remove HTML embedded characters\n",
    "df_old_added_neutral = df_old_added_neutral.replace(to_replace='@[^ ]+', value='', regex=True) # remove @usernames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "259b8dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tweets = []  \n",
    "new_labels = [] \n",
    "\n",
    "old_tweets = df_old.tweet.values\n",
    "old_labels = df_old.sentiment.values \n",
    "\n",
    "new_added_negative_tweets = [] \n",
    "new_added_negative_labels = [] \n",
    "\n",
    "old_added_neutral_tweets = [] \n",
    "old_added_neutral_labels = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74f1eb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_new.iterrows(): \n",
    "    new_tweets.append(row['tweet'])\n",
    "    if row['sentiment'] == float(1.0):\n",
    "        new_labels.append(1)\n",
    "    elif row['sentiment'] == float(2.0):\n",
    "        new_labels.append(2)\n",
    "    else: \n",
    "        new_labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bbc7486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_new_added_old_negative.iterrows(): \n",
    "    new_added_negative_tweets.append(row['tweet'])\n",
    "    if row['sentiment'] == float(1.0):\n",
    "        new_added_negative_labels.append(1)\n",
    "    elif row['sentiment'] == float(2.0):\n",
    "        new_added_negative_labels.append(2)\n",
    "    else: \n",
    "        new_added_negative_labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fabd6a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_old_added_neutral.iterrows(): \n",
    "    old_added_neutral_tweets.append(row['tweet'])\n",
    "    if row['sentiment'] == float(1.0):\n",
    "        old_added_neutral_labels.append(1)\n",
    "    elif row['sentiment'] == float(2.0):\n",
    "        old_added_neutral_labels.append(2)\n",
    "    else: \n",
    "        old_added_neutral_labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3740fe32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phil81/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2251: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# first we will run the new_tweets dataset \n",
    "\n",
    "input_ids = [] \n",
    "attention_masks = [] \n",
    "\n",
    "for i in range(len(new_tweets)):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        str(new_tweets[i]),                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 512,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'np',     # Return pytorch tensors.\n",
    "                   )\n",
    "    # add encoded sentence to list \n",
    "    input_ids.append(encoded_dict['input_ids'][0])\n",
    "    # add attention masks to list \n",
    "    attention_masks.append(encoded_dict['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6f21a272",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "\n",
    "for x in input_ids:\n",
    "    embeddings.append(x)\n",
    "    \n",
    "# print(len(embeddings))\n",
    "\n",
    "training_embeddings = embeddings[:45784]\n",
    "testing_embeddings = embeddings[45785:]\n",
    "\n",
    "training_labels = new_labels[:45784]\n",
    "testing_labels = new_labels[45785:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7bdc69b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=101)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = training_embeddings\n",
    "Y = training_labels\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=101)\n",
    "neigh.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "928ea1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = neigh.predict(testing_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "900c9936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.666\n",
      "Recall: 0.666\n",
      "Accuracy: 0.666\n",
      "F1 Score: 0.666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score \n",
    "print('Precision: %.3f' % precision_score(testing_labels, predictions, average='micro'))\n",
    "print('Recall: %.3f' % recall_score(testing_labels, predictions, average='micro'))\n",
    "print('Accuracy: %.3f' % accuracy_score(testing_labels, predictions))\n",
    "print('F1 Score: %.3f' % f1_score(testing_labels, predictions, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d60285aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6699429919402399"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(random_state=1, max_iter=200).fit(X, Y)\n",
    "clf.predict(testing_embeddings)\n",
    "clf.score(testing_embeddings, testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e79468de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of estimators -> 100\n",
      "Accuracy: 0.643\n",
      "Number of estimators -> 200\n",
      "Accuracy: 0.640\n",
      "Number of estimators -> 300\n",
      "Accuracy: 0.638\n",
      "Number of estimators -> 400\n",
      "Accuracy: 0.638\n",
      "Number of estimators -> 500\n",
      "Accuracy: 0.638\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "for i in [100,200,300,400,500]:\n",
    "    print(\"Number of estimators -> \" + str(i))\n",
    "    clf = RandomForestClassifier(max_depth = 5, random_state=0, n_estimators=i)\n",
    "    clf.fit(X, Y)\n",
    "    predictions = clf.predict(testing_embeddings)\n",
    "    print('Accuracy: %.3f' % accuracy_score(testing_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9eb3bdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phil81/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2251: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# now we will run old-tweets \n",
    "old_tweets = df_old.tweet.values \n",
    "old_labels = df_old.sentiment.values \n",
    "\n",
    "input_ids = [] \n",
    "attention_masks = [] \n",
    "\n",
    "for i in range(len(old_tweets)):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        str(old_tweets[i]),                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 512,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'np',     # Return pytorch tensors.\n",
    "                   )\n",
    "    # add encoded sentence to list \n",
    "    input_ids.append(encoded_dict['input_ids'][0])\n",
    "    # add attention masks to list \n",
    "    attention_masks.append(encoded_dict['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "11e02486",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "\n",
    "for x in input_ids:\n",
    "    embeddings.append(x)\n",
    "    \n",
    "# print(len(embeddings))\n",
    "\n",
    "training_embeddings = embeddings[:53550]\n",
    "testing_embeddings = embeddings[53551:]\n",
    "\n",
    "training_labels = old_labels[:53550]\n",
    "testing_labels = old_labels[53551:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ac15ee57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.290\n"
     ]
    }
   ],
   "source": [
    "X = training_embeddings\n",
    "Y = training_labels\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=101)\n",
    "neigh.fit(X, Y)\n",
    "\n",
    "predictions = neigh.predict(testing_embeddings)\n",
    "\n",
    "print('Accuracy: %.3f' % accuracy_score(testing_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b6e85956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05648008068582955"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(random_state=1, max_iter=200).fit(X, Y)\n",
    "clf.predict(testing_embeddings)\n",
    "clf.score(testing_embeddings, testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "df1545e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of estimators -> 100\n",
      "Accuracy: 0.002\n",
      "Number of estimators -> 200\n",
      "Accuracy: 0.002\n",
      "Number of estimators -> 300\n",
      "Accuracy: 0.002\n",
      "Number of estimators -> 400\n",
      "Accuracy: 0.002\n",
      "Number of estimators -> 500\n",
      "Accuracy: 0.002\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "for i in [100,200,300,400,500]:\n",
    "    print(\"Number of estimators -> \" + str(i))\n",
    "    clf = RandomForestClassifier(max_depth = 5, random_state=0, n_estimators=i)\n",
    "    clf.fit(X, Y)\n",
    "    predictions = clf.predict(testing_embeddings)\n",
    "    print('Accuracy: %.3f' % accuracy_score(testing_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6166f8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phil81/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2251: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# now we will run old-tweets-added-neutral \n",
    "input_ids = [] \n",
    "attention_masks = [] \n",
    "\n",
    "for i in range(len(old_added_neutral_tweets)):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        str(old_added_neutral_tweets[i]), # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 512,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'np',     # Return pytorch tensors.\n",
    "                   )\n",
    "    # add encoded sentence to list \n",
    "    input_ids.append(encoded_dict['input_ids'][0])\n",
    "    # add attention masks to list \n",
    "    attention_masks.append(encoded_dict['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c35857ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "\n",
    "for x in input_ids:\n",
    "    embeddings.append(x)\n",
    "    \n",
    "# print(len(embeddings))\n",
    "\n",
    "training_embeddings = embeddings[:73306]\n",
    "testing_embeddings = embeddings[73307:]\n",
    "\n",
    "training_labels = old_added_neutral_labels[:73306]\n",
    "testing_labels = old_added_neutral_labels[73307:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "778162c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.637\n"
     ]
    }
   ],
   "source": [
    "X = training_embeddings\n",
    "Y = training_labels\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X, Y)\n",
    "\n",
    "predictions = neigh.predict(testing_embeddings)\n",
    "\n",
    "print('Accuracy: %.3f' % accuracy_score(testing_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5bb5bb0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5082872928176796"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(random_state=1, max_iter=200).fit(X, Y)\n",
    "clf.predict(testing_embeddings)\n",
    "clf.score(testing_embeddings, testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d3060e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of estimators -> 1000\n",
      "Accuracy: 0.462\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "for i in [100,200,300,400,500]:\n",
    "    print(\"Number of estimators -> \" + str(i))\n",
    "    clf = RandomForestClassifier(max_depth = 5, random_state=0, n_estimators=i)\n",
    "    clf.fit(X, Y)\n",
    "    predictions = clf.predict(testing_embeddings)\n",
    "    print('Accuracy: %.3f' % accuracy_score(testing_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "241cdefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phil81/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2251: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# finally, we will now use new-tweets-added-old-negative\n",
    "input_ids = [] \n",
    "attention_masks = [] \n",
    "\n",
    "for i in range(len(new_added_negative_tweets)):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        str(new_added_negative_tweets[i]), # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 512,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'np',     # Return pytorch tensors.\n",
    "                   )\n",
    "    # add encoded sentence to list \n",
    "    input_ids.append(encoded_dict['input_ids'][0])\n",
    "    # add attention masks to list \n",
    "    attention_masks.append(encoded_dict['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "03c81496",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "\n",
    "for x in input_ids:\n",
    "    embeddings.append(x)\n",
    "    \n",
    "# print(len(embeddings))\n",
    "\n",
    "training_embeddings = embeddings[:61084]\n",
    "testing_embeddings = embeddings[61085:]\n",
    "\n",
    "training_labels = new_added_negative_labels[:61084]\n",
    "testing_labels = new_added_negative_labels[61085:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1bead088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.613\n"
     ]
    }
   ],
   "source": [
    "X = training_embeddings\n",
    "Y = training_labels\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=101)\n",
    "neigh.fit(X, Y)\n",
    "\n",
    "predictions = neigh.predict(testing_embeddings)\n",
    "\n",
    "print('Accuracy: %.3f' % accuracy_score(testing_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8329e70a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5899513776337115"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(random_state=1, max_iter=200).fit(X, Y)\n",
    "clf.predict(testing_embeddings)\n",
    "clf.score(testing_embeddings, testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2ef44dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of estimators -> 100\n",
      "Accuracy: 0.562\n",
      "Number of estimators -> 200\n",
      "Accuracy: 0.559\n",
      "Number of estimators -> 300\n",
      "Accuracy: 0.566\n",
      "Number of estimators -> 400\n",
      "Accuracy: 0.564\n",
      "Number of estimators -> 500\n",
      "Accuracy: 0.584\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "for i in [100,200,300,400,500]:\n",
    "    print(\"Number of estimators -> \" + str(i))\n",
    "    clf = RandomForestClassifier(max_depth = 5, random_state=0, n_estimators=i)\n",
    "    clf.fit(X, Y)\n",
    "    predictions = clf.predict(testing_embeddings)\n",
    "    print('Accuracy: %.3f' % accuracy_score(testing_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215da792",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
