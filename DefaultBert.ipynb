{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b24786f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 17:01:30.144623: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4de9f46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 17:01:31.305621: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-14 17:01:31.312448: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-03-14 17:01:31.360350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-14 17:01:31.360472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1070 computeCapability: 6.1\n",
      "coreClock: 1.7845GHz coreCount: 15 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 238.66GiB/s\n",
      "2022-03-14 17:01:31.360485: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-03-14 17:01:31.365668: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-03-14 17:01:31.365699: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-03-14 17:01:31.368084: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-03-14 17:01:31.370791: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-03-14 17:01:31.375601: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-03-14 17:01:31.377072: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-03-14 17:01:31.385258: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-03-14 17:01:31.385394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-14 17:01:31.385615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-14 17:01:31.385732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-03-14 17:01:31.385773: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-03-14 17:01:31.727344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-03-14 17:01:31.727362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-03-14 17:01:31.727367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-03-14 17:01:31.727540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-14 17:01:31.727707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-14 17:01:31.727813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-14 17:01:31.727903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/device:GPU:0 with 6904 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "2022-03-14 17:01:31.728205: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "# Get the GPU device name.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# The device name should look like the following:\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d23d120b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce GTX 1070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77a1532c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Sentiment Score')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "df = pd.read_csv('CSVFiles/training-data-positive-and-negative-sentiment-english.csv')\n",
    "df = df.drop('Unnamed: 0',axis=1)\n",
    "# df = df.iloc[:,[1,7]]\n",
    "# df = df[~df.Sentiment.str.contains(\"neutral\")]\n",
    "sns.countplot(x=df.Sentiment)\n",
    "plt.xlabel('Sentiment Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4901adbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33538</th>\n",
       "      <td>2019-05-21</td>\n",
       "      <td>My prediction for $btc. this week it will brea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40763</th>\n",
       "      <td>2019-05-21</td>\n",
       "      <td>Start on: 2d 05H 29m 44s! #TRON #TRX $TRX #BTT...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14699</th>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>世界取引所の円換算ビットコイン価格をリアルタイム表示 #ビットコイン #Bitcoin \"h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41582</th>\n",
       "      <td>2019-05-21</td>\n",
       "      <td>Can anyone make #NPXS $1 soon? @PundiXLabs @Pu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13023</th>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>Las 5 criptomonedas más valoradas en la semana...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57622</th>\n",
       "      <td>2019-05-21</td>\n",
       "      <td>A new post: Bitcoin Rebounds To $7,900, Why An...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26864</th>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>@IndodaxOfficial kapan min semua koin pair den...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43237</th>\n",
       "      <td>2019-05-21</td>\n",
       "      <td>@binance 100K USD Per BTC and then 1 Million P...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6337</th>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>@cryptocurrency \\n\\nMax Keiser Says $28K Bitco...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5677</th>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>adakah mutualku yg dateng kesini?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date                                               text  \\\n",
       "33538  2019-05-21  My prediction for $btc. this week it will brea...   \n",
       "40763  2019-05-21  Start on: 2d 05H 29m 44s! #TRON #TRX $TRX #BTT...   \n",
       "14699  2019-05-27  世界取引所の円換算ビットコイン価格をリアルタイム表示 #ビットコイン #Bitcoin \"h...   \n",
       "41582  2019-05-21  Can anyone make #NPXS $1 soon? @PundiXLabs @Pu...   \n",
       "13023  2019-05-27  Las 5 criptomonedas más valoradas en la semana...   \n",
       "57622  2019-05-21  A new post: Bitcoin Rebounds To $7,900, Why An...   \n",
       "26864  2019-05-27  @IndodaxOfficial kapan min semua koin pair den...   \n",
       "43237  2019-05-21  @binance 100K USD Per BTC and then 1 Million P...   \n",
       "6337   2019-05-27  @cryptocurrency \\n\\nMax Keiser Says $28K Bitco...   \n",
       "5677   2019-05-27                  adakah mutualku yg dateng kesini?   \n",
       "\n",
       "       Sentiment  \n",
       "33538          0  \n",
       "40763          0  \n",
       "14699          1  \n",
       "41582          0  \n",
       "13023          1  \n",
       "57622          0  \n",
       "26864          1  \n",
       "43237          0  \n",
       "6337           1  \n",
       "5677           1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_label = {'Negative': 0, 'Positive': 1}\n",
    "df['Sentiment'] = df['Sentiment'].map(encoded_label)\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74db71b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer \n",
    "\n",
    "# load the BERT tokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e30f5c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  È appena uscito un nuovo video! LES CRYPTOMONNAIES QUI PULVÉRISENT BITCOIN EN 2019 \n",
      "Tokenized:  ['e', 'app', '##ena', 'usc', '##ito', 'un', 'nu', '##ovo', 'video', '!', 'les', 'crypt', '##omo', '##nna', '##ies', 'qui', 'pu', '##lver', '##isen', '##t', 'bit', '##co', '##in', 'en', '2019']\n",
      "Token IDs:  [1041, 10439, 8189, 15529, 9956, 4895, 16371, 16059, 2678, 999, 4649, 19888, 19506, 9516, 3111, 21864, 16405, 26229, 28992, 2102, 2978, 3597, 2378, 4372, 10476]\n"
     ]
    }
   ],
   "source": [
    "# some regex to clean the data before passing it to BERT tokenizer  \n",
    "import re \n",
    "df = df.replace(to_replace='https?:\\/\\/\\S+', value='', regex=True) # remove https urls \n",
    "df = df.replace(to_replace=\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", value='', regex=True) # remove www urls \n",
    "df = df.replace(to_replace=\"\\[video\\]\", value='', regex=True) # remove videos that got replaced with [video]\n",
    "df = df.replace(to_replace='{link}', value='', regex=True) # remove links \n",
    "df = df.replace(to_replace='&[a-z]+;', value='', regex=True) # remove HTML embedded characters\n",
    "df = df.replace(to_replace='@[^ ]+', value='', regex=True) # remove @usernames \n",
    "\n",
    "# some sample output \n",
    "tweets = df.text.values \n",
    "labels = df.Sentiment.values \n",
    "# import numpy as np\n",
    "# labels = np.array(labels)\n",
    "\n",
    "print(' Original: ', tweets[0])\n",
    "print('Tokenized: ', tokenizer.tokenize(tweets[0]))\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(tweets[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "227e362f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  È appena uscito un nuovo video! LES CRYPTOMONNAIES QUI PULVÉRISENT BITCOIN EN 2019 \n",
      "Tokenized:  ['e', 'app', '##ena', 'usc', '##ito', 'un', 'nu', '##ovo', 'video', '!', 'les', 'crypt', '##omo', '##nna', '##ies', 'qui', 'pu', '##lver', '##isen', '##t', 'bit', '##co', '##in', 'en', '2019']\n",
      "Token IDs:  [1041, 10439, 8189, 15529, 9956, 4895, 16371, 16059, 2678, 999, 4649, 19888, 19506, 9516, 3111, 21864, 16405, 26229, 28992, 2102, 2978, 3597, 2378, 4372, 10476]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer \n",
    "\n",
    "# load the BERT tokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "import re \n",
    "df = df.replace(to_replace='https?:\\/\\/\\S+', value='', regex=True) # remove https urls \n",
    "df = df.replace(to_replace=\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", value='', regex=True) # remove www urls \n",
    "df = df.replace(to_replace=\"\\[video\\]\", value='', regex=True) # remove videos that got replaced with [video]\n",
    "df = df.replace(to_replace='{link}', value='', regex=True) # remove links \n",
    "df = df.replace(to_replace='&[a-z]+;', value='', regex=True) # remove HTML embedded characters\n",
    "df = df.replace(to_replace='@[^ ]+', value='', regex=True) # remove @usernames \n",
    "\n",
    "# some sample output \n",
    "tweets = df.text.values \n",
    "labels = df.Sentiment.values \n",
    "# import numpy as np\n",
    "# labels = np.array(labels)\n",
    "\n",
    "print(' Original: ', tweets[0])\n",
    "print('Tokenized: ', tokenizer.tokenize(tweets[0]))\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(tweets[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d591754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence length:  226\n"
     ]
    }
   ],
   "source": [
    "max_len = 0 \n",
    "\n",
    "for i in range(len(tweets)):\n",
    "    # tokenize the text and add `[CLS]` and `[SEP]` tokens \n",
    "    input_ids = tokenizer.encode(str(tweets[i]), add_special_tokens=True)\n",
    "    \n",
    "    #update max sentence length\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "    \n",
    "print('max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2560d3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phil81/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2251: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  È appena uscito un nuovo video! LES CRYPTOMONNAIES QUI PULVÉRISENT BITCOIN EN 2019 \n",
      "Token IDs: tensor([  101,  1041, 10439,  8189, 15529,  9956,  4895, 16371, 16059,  2678,\n",
      "          999,  4649, 19888, 19506,  9516,  3111, 21864, 16405, 26229, 28992,\n",
      "         2102,  2978,  3597,  2378,  4372, 10476,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "# we will set max_len to 235 \n",
    "# now we can perform tokenization \n",
    "input_ids = [] \n",
    "attention_masks = [] \n",
    "\n",
    "for i in range(len(tweets)):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        str(tweets[i]),                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    # add encoded sentence to list \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    # add attention masks to list \n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "#convert the lists into tensors \n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "print('Original: ', tweets[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d41ef0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53,550 training samples\n",
      "5,950 validation samples\n"
     ]
    }
   ],
   "source": [
    "# we will now divide our training data into training and validation sets (90/10)\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# set batch size to 32 for fine-tuning BERT \n",
    "batch_size = 32 \n",
    "\n",
    "# combine training inputs into a training dataset \n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "training_size = int(0.9*len(dataset))\n",
    "validation_size = len(dataset) - training_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [training_size, validation_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(training_size))\n",
    "print('{:>5,} validation samples'.format(validation_size))\n",
    "# THIS CONCLUDES PROCESSING OUR DATA FOR TRAINING \n",
    "# https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=6J-FYdx6nFE_ \n",
    "\n",
    "# create an iterator for our dataset using the torch DataLoader class, saving on memory during training and boost training speed \n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                                 sampler=RandomSampler(train_dataset), \n",
    "                                 batch_size=batch_size)\n",
    "validation_dataloader = DataLoader(val_dataset, \n",
    "                                   sampler=RandomSampler(val_dataset), \n",
    "                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e247b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels    = 2, \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0dea5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>Right.  This isn't your grandpa's Bitcoin....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>Bitcoin Price Prediction Today: Daily (BTC) Va...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-05-21</td>\n",
       "      <td>Most experts say that  BTC is  bullish again...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-05-21</td>\n",
       "      <td>資金が循環しきっていないのでまだもう少しかかりそうです。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>#Blockchain #cryptocurrency #binance #Bitcoin ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>is being traded on CoinExchange\\nYou can buy ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>2019-05-21</td>\n",
       "      <td>VanEck Bitcoin (BTC) ETF Verdict delayed by th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>BTC、吹いてる🎵</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>2019-05-21</td>\n",
       "      <td>Let us know if you need a Bitcoin Fork (pen)!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>$BTC\\n強い奴が勝ってる相場やね</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date                                               text  Sentiment\n",
       "0     2019-05-27      Right.  This isn't your grandpa's Bitcoin....          1\n",
       "1     2019-05-27  Bitcoin Price Prediction Today: Daily (BTC) Va...          1\n",
       "2     2019-05-21    Most experts say that  BTC is  bullish again...          0\n",
       "3     2019-05-21                       資金が循環しきっていないのでまだもう少しかかりそうです。          0\n",
       "4     2019-05-27  #Blockchain #cryptocurrency #binance #Bitcoin ...          1\n",
       "...          ...                                                ...        ...\n",
       "4995  2019-05-27   is being traded on CoinExchange\\nYou can buy ...          1\n",
       "4996  2019-05-21  VanEck Bitcoin (BTC) ETF Verdict delayed by th...          0\n",
       "4997  2019-05-27                                          BTC、吹いてる🎵          1\n",
       "4998  2019-05-21    Let us know if you need a Bitcoin Fork (pen)!            0\n",
       "4999  2019-05-27                                 $BTC\\n強い奴が勝ってる相場やね          1\n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsample = df.sample(n=5000, ignore_index=True)\n",
    "dfsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30aeddcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phil81/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2251: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tweets = dfsample.text.values \n",
    "labels = dfsample.Sentiment.values \n",
    "\n",
    "input_ids = [] \n",
    "attention_masks = [] \n",
    "\n",
    "for i in range(len(tweets)): \n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        str(tweets[i]),\n",
    "        add_special_tokens = True, \n",
    "        max_length = 64, \n",
    "        pad_to_max_length=True, \n",
    "        return_attention_mask = True, \n",
    "        return_tensors = 'pt',\n",
    "    )\n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "540d704e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 5,000 test sentences...\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# evaluate on our test set \n",
    "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
    "\n",
    "# put model in evaluation mode \\\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# variables for tracking labels \n",
    "predictions, true_labels = [], []\n",
    "\n",
    "for batch in prediction_dataloader:\n",
    "    # add batch to GPU \n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    \n",
    "    # unpack inputs from dataloader \n",
    "    b_input_ids, b_input_mask, b_labels = batch \n",
    "    \n",
    "    # tell model to not compute/store gradients \n",
    "    # saves memory and speeds up transaction \n",
    "    with torch.no_grad():\n",
    "        # forward pass \n",
    "        result = model(b_input_ids,\n",
    "                      token_type_ids=None,\n",
    "                      attention_mask = b_input_mask,\n",
    "                      return_dict=True)\n",
    "        \n",
    "    logits = result.logits \n",
    "    # move logits and labels to CPU \n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "    \n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aee35898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples: 2530 of 5000 (50.60%)\n"
     ]
    }
   ],
   "source": [
    "print('Positive samples: %d of %d (%.2f%%)' % (dfsample.Sentiment.sum(), len(dfsample.Sentiment), (dfsample.Sentiment.sum() / len(dfsample.Sentiment) * 100.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755595a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
